{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting rooftop available surface for installing PV modules in aerial images using Machine Learning\n",
    "In this notebook we will present the entire pipeline to train a Unet model from a desired data set, evaluate the results and visualize the predictions. We present multiple ways to initialize and train a Unet. All the methods are availble under the section *Training Methods*. Note that in our results *Adaptative Training* provided the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot  as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms.functional import normalize\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from train.train import *\n",
    "from tempfile import TemporaryFile\n",
    "from process_data.normalize import * \n",
    "\n",
    "from model.unet import *\n",
    "from loss.loss import *\n",
    "from process_data.data_loader import *\n",
    "from hyperparameters.select_param import *\n",
    "from process_data.import_test import *\n",
    "from plots.plots import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed_torch() # For reproducibility we set the seed with a seed_torch() method that set the seed in numpy and pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data Set\n",
    "First we load the data set that we will use for training. Each sample is an image with its mask (label). An image is represented as a 3x250x250 array with each of the 3 color chanel being 250x250 pixels. The asssociated mask is a 250x250 array. Note that we already split the images in train/val/test 80/10/10 in advance to make our reproducibility as clear as possible.\n",
    "\n",
    "We perform data augmentation and transformation on the training set to counter the low amount of images in our data set. However in the validation set and test set, we only perform transformatio and no augmentation. Again this is to make reproducibility easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_train_image = 'data/train/images/all'\n",
    "folder_path_train_masks = 'data/train/labels/all'\n",
    "folder_path_test_image = 'data/test/images/all'\n",
    "folder_path_test_masks = 'data/test/labels/all'\n",
    "folder_path_val_image = 'data/val/images/all'\n",
    "folder_path_val_masks = 'data/val/labels/all'\n",
    "\n",
    "# Load dataset\n",
    "train_set = DataLoaderSegmentation(folder_path_train_image,folder_path_train_masks) # 80%\n",
    "test_set = DataLoaderSegmentation(folder_path_test_image,folder_path_test_masks,augment=False)# 10%, no augmentation\n",
    "val_set = DataLoaderSegmentation(folder_path_val_image,folder_path_val_masks,augment=False) # 10%, no augmentation\n",
    "\n",
    "# Init data loader\n",
    "train_loader = DataLoader(train_set,batch_size=2, shuffle=True ,num_workers=0)\n",
    "val_loader = DataLoader(val_set,batch_size=2, shuffle=True ,num_workers=0)\n",
    "test_loader = DataLoader(test_set,batch_size=2 , shuffle=True ,num_workers=0)\n",
    "print(len(train_set),len(test_set),len(val_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = UNet(3,1,False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We may compute the mean and standard deviation of the train loader. This is used either to check if the data loader is normalized, or to compute the mean and std for the normalizer in data_loader.\n",
    "\n",
    "mean_std(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Methods\n",
    "We now present a certain number of training methods, they all initialize a new Unet model from scratch and train it. Note that this methods should not be used simultaneously. Instead you should choose one of the methods, run it and evaluate its performance.\n",
    "\n",
    "## Regular training\n",
    "This is a simple training loop. We can tune the num_epochs, the learning rate and the parameter of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Init training parameters\n",
    "num_epochs = 50\n",
    "model = UNet(3,1,False).to(device)\n",
    "loss_function = torch.nn.BCEWithLogitsLoss(weight=torch.FloatTensor([4]).cuda())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train model\n",
    "history_train_loss, history_val_loss, history_train_iou, history_val_iou = training_model(train_loader,loss_function,optimizer,model,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(-0.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the evolution of the loss and the IoU, either on the train or validation set.\n",
    "plot_train_val(history_train_loss,history_val_loss,period=25, al_param=False, metric='loss')\n",
    "plot_train_val(history_train_iou,history_val_iou,period=25, al_param=False, metric='IoU')"
   ]
  },
  {
   "source": [
    "## Training with adaptative learning rate\n",
    "Training with adaptative learning rate is a regular training with an added learning rate scheduler. The task of the scheduler is to change the learning rate depending of the number of epochs. In our testing, the linear learning rate scheduler provided the best results. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init training parameters\n",
    "num_epochs = (150)\n",
    "loss_function = torch.nn.BCEWithLogitsLoss(weight=torch.FloatTensor([15]).cuda())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00008)\n",
    "# We opted for the linear scheduler. For example, every 60 epochs the learning rate is multiplied by 0.8.\n",
    "al_param=60\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, al_param, gamma=0.8, last_epoch=-1, verbose=False)\n",
    "\n",
    "# Train model\n",
    "history_train_loss, history_val_loss, history_train_iou, history_val_iou = training_model(train_loader,loss_function,optimizer,model,num_epochs,scheduler,val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the evolution of the loss and the IoU, either on the train or validation set.\n",
    "plot_train_val(history_train_loss,history_val_loss,period=25, al_param=al_param, metric='loss')\n",
    "plot_train_val(history_train_iou,history_val_iou,period=25, al_param=al_param, metric='IoU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptative Learning\n",
    "Performs a training on a model over a training data set by doing the following: we first fix the learing rate, then we split the training set into two folds, the model is trained on the first fold then on the second fold. After this has been done, we move on the next learning rate.\n",
    "\n",
    "Note that this method as no theoritical basis and came up from a flawed cross validation method we did not implemented correctly. However since it was providing good results we decided to clean it and keep it as a training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Init training parameters\n",
    "lr_candidates = np.logspace(-1,-2,num=5)\n",
    "num_epochs = 5\n",
    "loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([6]).cuda())\n",
    "model = UNet(3,1,False).to(device)\n",
    "\n",
    "# Train model\n",
    "best_iou, history_iou = adptative_learning(train_set,val_loader,loss_function,best_model,num_epochs,lr_candidates)\n"
   ]
  },
  {
   "source": [
    "# Miscellaneous methods\n",
    "We may try to identify the best learning rate for our task, either by using a regular grid search algorithm on the learning rate or performing cross validation to have a good estimate of the IoU we may get.\n",
    "\n",
    "## Find Best Learning Rate\n",
    "We perform a grid search algorithm on the learning rate, by using a predefined range of learning rates lr and keeping the model that has the best IoU score."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([25]).cuda())\n",
    "n_splits = 2\n",
    "num_epochs = 300\n",
    "lr = np.logspace(-1,-2,num=5)\n",
    "\n",
    "best_lr, best_iou = select_hyper_param(train_loader,n_splits,loss_function,num_epochs,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "We can perform K-fold cross validation on our training set, to have an estimate of the mean iou and accuracy we may hope to achieve. Note that K-Fold CV is not considered as appropriate for CNN parameters tuning, since it is very costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([6]).cuda())\n",
    "n_splits = 2\n",
    "num_epochs = 10\n",
    "lr = 0.01\n",
    "\n",
    "iou_, acc_ = cross_validation(train_dataset=train_set,n_splits, loss_function, num_epochs, lr)"
   ]
  },
  {
   "source": [
    "# Export or import a model\n",
    "\n",
    "## Export a model \n",
    "To save a model trained with one of the aforementionned methods, one could use the following cell. The model is save in the *model* folder."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/'+input('Name of the model file:')+\".pt\")"
   ]
  },
  {
   "source": [
    "## Import a model \n",
    "To import a model, one could use the following cell. The model must be located in the *model* folder. In the command prompt you can for example write *all/batch5loss4_200*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'model/'+input('Name of the model file:')+\".pt\"\n",
    "model = UNet(3,1,False).to(device)\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "source": [
    "# Evaluation of the model\n",
    "We can evaluate the model to have the mean (IoU, Accuracy) on every data set, and print the number of parameters of the Unet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "print('Train:', test_model(train_loader,model))\n",
    "print('Val:', test_model(val_loader,model))\n",
    "print('Test:', test_model(test_loader,model))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the model\n",
    "With the model we trained or imported, we can display from the test_loader examples of its prediction."
   ]
  },
  {
   "source": [
    "# Get the input, transformed input and prediction made by the model\n",
    "model.eval()\n",
    "index_random_sample = int(np.random.random()*len(test_loader.dataset))\n",
    "(x,y,z) = test_loader.dataset.__getitem__(index_random_sample,show_og=True)\n",
    "ypred = torch.squeeze(model.predict(torch.unsqueeze(x,0).cuda())).cpu().detach().numpy()\n",
    "\n",
    "# Display all 4 images\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 7, forward=True)\n",
    "ax1 = fig.add_subplot(1,4,1)\n",
    "ax1.title.set_text('Input Image')\n",
    "ax2 = fig.add_subplot(1,4,2)\n",
    "ax2.title.set_text('Transformed Input Image')\n",
    "ax3 = fig.add_subplot(1,4,3)\n",
    "ax3.title.set_text('Expected Mask')\n",
    "ax4 = fig.add_subplot(1,4,4)\n",
    "ax4.title.set_text('Predicted Mask')\n",
    "\n",
    "ax1.imshow(z)\n",
    "ax2.imshow(np.transpose(x.numpy(),(1,2,0)))\n",
    "ax3.imshow(y)\n",
    "ax4.imshow(np.around((ypred)))\n",
    "\n",
    "# Compute IoU and accuracy on prediction and mask\n",
    "predict_iou = np.around(iou(np.around(ypred),y.numpy()),4)\n",
    "predict_acc = accuracy(np.around(ypred),y.numpy())\n",
    "print('Iou:',predict_iou)\n",
    "print('Accuracy:', predict_acc)\n"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Display an unseen image"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_and_show(model,'test.PNG') # Note that 'test.png' should be located in the root of the folder"
   ]
  },
  {
   "source": [
    "# Paper thing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "arr = []\n",
    "dict = {'key':'array'}\n",
    "\n",
    "for idx, (x,y) in enumerate(test_set):\n",
    "    #ypred = torch.squeeze(model.predict(torch.unsqueeze(x,0).cuda())).cpu().detach().numpy()\n",
    "    ypred = y.cpu().detach().numpy()\n",
    "\n",
    "    arr.append(np.around(ypred))\n",
    "    im = Image.fromarray(np.around(ypred))\n",
    "\n",
    "dict[0] = os.listdir('data/paper/')\n",
    "dict[1] = arr\n",
    "\n",
    "pickle.dump(dict,open(\"predictions_area.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('predictions_area.pkl','rb')\n",
    "dict = pickle.load(f)\n",
    "f.close()\n",
    "dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python376jvsc74a57bd01225fc73a0ee5aca32e4f5bfaaa1548f2086c2ff8fe8ba946b9466c0ff4e1236",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}